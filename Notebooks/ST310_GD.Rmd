---
title: "Apollo ST310 Group Project"
output:
  html_document:
    toc: yes
date: "21-03-2023"
---

# Import libraries

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(yardstick)
library(tidymodels)
library(skimr)
library(ranger)
library(vip)
```
# EDA & Data Manipulation

## Import data

```{r}
# Import and view head of training data
path <- "/Users/bbste/Documents/LSE/ST310/ST310-Group-Project/Data/Stellar-Classification-Dataset.csv"
raw_df <- read.csv(path)
```

```{r}
# Make "Class" the first column
#reordered_raw_df <- raw_df[,c(14,1:13, 15:ncol(raw_df))]
#head(reordered_raw_df)
```

## Distribution of `class`

```{r}
ggplot(raw_df,aes(x=factor(class))) +
geom_bar() + 
labs(title="Counts of Class Column", x="Class", y = "Count")+
geom_text(aes(label=..count..),stat='count', vjust=-0.2)

```
```{r}
raw_df <- raw_df[,c(14,1:13, 15:ncol(raw_df))]
head(raw_df)
```



```{r}
# Filter the data by class and subsample
#galaxy_df <- reordered_raw_df[reordered_raw_df$class == "GALAXY", ]
#subsampled_galaxy_df <- galaxy_df[sample(nrow(galaxy_df), size = 1000, replace = FALSE),]

#qso_df <- reordered_raw_df[reordered_raw_df$class == "QSO", ]
#subsampled_qso_df <- qso_df[sample(nrow(qso_df), size = 1000, replace = FALSE),]

#star_df <- reordered_raw_df[reordered_raw_df$class == "STAR", ]
#subsampled_star_df <- star_df[sample(nrow(star_df), size = 1000, replace = FALSE),]
```

```{r}
# Create new DataFrame
#new_df <- rbind(subsampled_galaxy_df, subsampled_qso_df, subsampled_star_df)
#head(new_df)
```

## Re-labelling `class`

```{r}
raw_df[raw_df == 'QSO' | raw_df == 'STAR'] <- 'OTHER'
```

```{r}
ggplot(raw_df,aes(x=factor(class))) +
geom_bar() + 
labs(title="Counts of Class Column", x="Class", y = "Count")+
geom_text(aes(label=..count..),stat='count', vjust=-0.2)
```

## Subsample the data

```{r}
# Filter the data by class and subsample
galaxy_df <- raw_df[raw_df$class == "GALAXY", ]
subsampled_galaxy_df <- galaxy_df[sample(nrow(galaxy_df), size = 40000, replace = FALSE),]

other_df <- raw_df[raw_df$class == "OTHER", ]
subsampled_other_df <- other_df[sample(nrow(other_df), size = 40000, replace = FALSE),]
```

```{r}
# Create new DataFrame
df <- rbind(subsampled_galaxy_df, subsampled_other_df)
dim(df)
```

```{r}
ggplot(df,aes(x=factor(class))) +
geom_bar() + 
labs(title="Counts of Class Column", x="Class", y = "Count")+
geom_text(aes(label=..count..),stat='count', vjust=-0.2)

```


## Summary statistics

```{r}
df %>% 
  skimr::skim(colnames(df))
```
# Model Recipe

## Train/Test Split

```{r}
set.seed(222)
# Put 80% of the data into the training set 
data_split <- initial_split(df, prop = 0.8)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

## Create our recipe

```{r}
# Declare the ID variables
IDs <- c("obj_ID", "run_ID", "rerun_ID", "cam_col", "field_ID", "spec_obj_ID", "plate", "MJD", "fiber_ID")
```

```{r}
# Define our model, and exclude the ID variables
recipe <- train_data %>%
  recipe(class ~ .) %>% 
  update_role(all_of(IDs), new_role = "ID") %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep()
```
```{r}
test_data <- recipe %>%
  bake(test_data)
```

```{r}
train_data <- juice(recipe)
```

```{r}
# Summary of the recipe
summary(recipe)
```

# Logisitic Regression Model

```{r}
# Define our logistic regression model
logistic_model <- 
  logistic_reg() %>% 
  set_engine("glm")
```

## Workflow

```{r}
# Create our workflow
logistic_wflow <- 
  workflow() %>% 
  add_model(logistic_model) %>% 
  add_recipe(recipe)

logistic_wflow
```

## Fit and explore the model

```{r}
# Fit the model
logistic_fit <- 
  logistic_wflow %>% 
  fit(data = train_data)
```

```{r}
logistic_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

```{r}
logistic_augment <- 
  augment(logistic_fit, test_data, type = "prob")
```

## Predictions

```{r}
# DataFrame of prediction probabilities
pred_df <- logistic_augment %>%
  select(class, .pred_class, .pred_GALAXY)
```

```{r}
pred_df
```


```{r}
# Distribution of predictions
ggplot(pred_df,aes(x=factor(class))) +
geom_bar() + 
labs(title="Counts of Class Column", x="Class", y = "Count")+
geom_text(aes(label=..count..),stat='count', vjust=-0.2)
```

### Evaluation metric: ROC/AUC

```{r}
logistic_augment %>% 
  roc_curve(truth = as.factor(class), .pred_GALAXY) %>% 
  autoplot()
```
```{r}
# Area Under Curve (AUC)
logistic_augment %>% 
  roc_auc(truth = as.factor(class), .pred_GALAXY)
```
### Evaluation Metric: Accuracy Score

```{r}
accuracy(pred_df, as.factor(class), as.factor(.pred_class))
```

```{r}
binary_class_df <- df
binary_class_df[binary_class_df == 'GALAXY'] <- 1
binary_class_df[binary_class_df == 'OTHER'] <- 0
#y <- as.factor(df$class)
head(binary_class_df)
```

```{r}
# Create a matrix of predictor variables 
X <- binary_class_df[,c(3:9, 15)]
# Standardise all variables so units are more interpretable
X <- as.matrix(scale(X[1:8]))
#X <- as.matrix(X)
head(X, 1)
```

```{r}
Y <- binary_class_df$class
Y <- as.numeric(Y)
Y <- as.matrix(Y)
```


```{r}
sigmoid <- function(x){
   return( 1/(1+exp(-x)) )
}
```

```{r}
# Define the likelihood function
neglogL <- function(X, Y, beta) {
  sigmoid <- 1/(1+exp(-X %*% beta))
  # Use loss function minus Binary Cross Entropy 
  -sum(Y * log(sigmoid) + (1-Y) * log(1-sigmoid))
}
```

```{r}
numeric_grad <- function(X, Y, beta, h = 1e-06) {
  # Approximate each coordinate of the gradient
  numerator <- sapply(1:length(beta), function(j) {
    H <- rep(0, length(beta))
    H[j] <- h # step in coordinate j
    neglogL(X, Y, beta + H) - neglogL(X, Y, beta - H)
  })
  numerator / (2*h)
}
```

```{r}
max_steps <- 50 # try ~3 for comparison
beta_prev2 <- c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1) # random start
#beta_prev2 <- c(mean(Y), cor(X[,-1], Y)) # "warm" start
grad_prev2 <- numeric_grad(X, Y, beta_prev2)
beta_prev1 <- beta_prev2 + 0.1 * grad_prev2 / sqrt(sum(grad_prev2^2))
grad_prev1 <- numeric_grad(X, Y, beta_prev1)
previous_loss <- neglogL(X, Y, beta_prev2)
next_loss <- neglogL(X, Y, beta_prev1)
steps <- 1
next_loss
```

```{r}
#previous_loss - 
previous_loss - next_loss
```


```{r}
tol <- 10 # (error) tolerance
while(abs(previous_loss - next_loss) > tol){
  # Compute BB step size
  grad_diff <- grad_prev1 - grad_prev2
  step_BB <- sum((beta_prev1 - beta_prev2) * grad_diff) / sum(grad_diff^2)
  beta_prev2 <- beta_prev1
  
  # Update step
  beta_prev1 <- beta_prev1 - abs(step_BB) * grad_prev1
  
  # Shift previous steps
  grad_prev2 <- grad_prev1
  grad_prev1 <- numeric_grad(X, Y, beta_prev1)  
  previous_loss <- next_loss
  next_loss <- neglogL(X, Y, beta_prev1)
  
  print(previous_loss - next_loss)
  
  # Print loss every 5 steps, track path, control loop
  #if (round(steps/5) == steps/5) print(previous_loss)
  #steps <- steps + 1
  #beta_path[steps, 2] <- next_loss
  #beta_path[steps, 3:ncol(beta_path)] <- beta_prev1
  #if (steps > max_steps) break
}
```

# Step 3 Tree Model + Interpretation




# Random Forest Model

## Create model
```{r}
rf_model <- rand_forest(mode = "classification", trees = 20) %>% 
  set_engine("ranger", importance = "impurity")
```

```{r}
rf_workflow <- 
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(recipe)
```

## Fit model and examine variable importance

```{r}
rf_workflow %>% 
  fit(df) %>%
  extract_fit_parsnip() %>% 
  vip(num_features = 8) +
  labs(title = "Random forest variable importance")
```

## Predictions

```{r}
# Fit the model
rf_fit <- 
  rf_workflow %>% 
  fit(data = train_data)
```


```{r}
rf_augment <- 
  augment(rf_fit, test_data, type = "prob")
```

```{r}
# DataFrame of prediction probabilities
rf_pred_df <- rf_augment %>%
  select(class, .pred_class, .pred_GALAXY)
```

```{r}
rf_pred_df
```


### Evaluation metric: ROC/AUC

```{r}
rf_augment %>% 
  roc_curve(truth = as.factor(class), .pred_GALAXY) %>% 
  autoplot()
```

```{r}
# Area Under Curve (AUC)
rf_augment %>% 
  roc_auc(truth = as.factor(class), .pred_GALAXY)
```

### Evaluation Metric: Accuracy Score

```{r}
accuracy(rf_pred_df, as.factor(class), as.factor(.pred_class))
```
### 10-fold Cross-Validation

```{r}
folds <- vfold_cv(train_data, v = 10)
```

```{r}
rf_random_samples <- 
  rf_workflow %>% 
  fit_resamples(folds)
```

```{r}
rf_random_samples
```

```{r}
collect_metrics(rf_random_samples)
```














